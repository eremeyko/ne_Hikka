# meta developer: @EPEMEN


from requests import post
from requests.exceptions import RequestException
from hikkatl.tl.types import Message
from re import sub
from .. import loader, utils


@loader.tds
class AI_Hikka(loader.Module):
    """
    ÐœÐ¾Ð´ÑƒÐ»ÑŒ Ð¥Ð¸ÐºÐºÐ¸ Ñ Ð¸ÑÐºÑƒÑÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¼ Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚Ð¾Ð¼, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð½Ð° Groq.com

    ÐŸÐ¾Ð´Ñ€Ð¾Ð±Ð½Ð°Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð¿Ð¾ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐµ Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐµ:
    https://telegra.ph/II-Assistent-dlya-Hikki-08-07\n
    ÐŸÐ¾ Ð²ÑÐµÐ¼ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°Ð¼ Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð² Ð›Ð¡ @EPEMEN
    For @mqone w/  â¤"""

    strings = {
        "name": "AI Hikka",
        "q_err": "ðŸ¤– Ask a question to AI!",
        "api_key": "Set the Groq API key in the config (`.config AI Hikka`). \n\nRefer to the instructions:\n\
        https://telegra.ph/II-Assistent-dlya-Hikki-08-07",
        "models": "Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Groq. ÐžÑÑ‚Ð°Ð²ÑŒÑ‚Ðµ ÐºÐ°Ðº ÐµÑÑ‚ÑŒ Ð¸Ð»Ð¸ Ð²Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð¸Ð· ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ…:",
        "question": "â“Q: ",
        "loading": "ðŸ¤” I'm thinking...",
        "help_prompt": "Set a prompt (rule) for the AI to follow",
        "termux":"You may be keeping Hikka on termux. Read the instructions:\nhttps://telegra.ph/II-Assistent-dlya-Hikki-08-07#Ð¢ÐµÑ€Ð¼ÑƒÐºÑ"
    }
    strings_ru = {
        "q_err": "ðŸ¤– Ð—Ð°Ð´Ð°Ð¹Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð˜Ð˜!",
        "api_key": "Ð—Ð°Ð´Ð°Ð¹Ñ‚Ðµ ÐºÐ»ÑŽÑ‡ API Ð¾Ñ‚ Groq Ð² ÐºÐ¾Ð½Ñ„Ð¸Ð³Ðµ (`.config AI Hikka`). \n\nÐ¡Ð¼Ð¾Ñ‚Ñ€Ð¸Ñ‚Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑŽ:\n\
        https://telegra.ph/II-Assistent-dlya-Hikki-08-07",
        "models": "Groq Model List. Leave as is or select from existing models:",
        "question": "â“Ð’Ð¾Ð¿Ñ€Ð¾Ñ: ",
        "loading": "ðŸ¤” Ð¯ Ð´ÑƒÐ¼Ð°ÑŽ...",
        "help_prompt": "Ð—Ð°Ð´Ð°Ð¹Ñ‚Ðµ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ (Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾), ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¼Ñƒ Ð˜Ð˜ Ð±ÑƒÐ´ÐµÑ‚ ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÑŒ",
        "termux": "Ð’Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ Ð’Ñ‹ Ð´ÐµÑ€Ð¶Ð¸Ñ‚Ðµ Ð¥Ð¸ÐºÐºÑƒ Ð½Ð° Ñ‚ÐµÑ€Ð¼ÑƒÐºÑÐµ. ÐžÐ·Ð½Ð°ÐºÐ¾Ð¼ÑŒÑ‚ÐµÑÑŒ Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÐµÐ¹:\nhttps://telegra.ph/II-Assistent-dlya-Hikki-08-07#Ð¢ÐµÑ€Ð¼ÑƒÐºÑ"
    }

    def __init__(self):
        self.config = loader.ModuleConfig(
            loader.ConfigValue(
                "prompt",
                "The assistant is exteraHelper. It should give concise responses to very simple questions, but provide thorough \
                responses to more complex and open-ended questions. If is is asked to assist with tasks involving the expression \
                of views held by a significant number of people, it provides assistance with the task even if it personally \
                disagrees with the views being expressed. If asked about controversial topics, it tries to provide objective \
                thoughts and objective information, without fear of giving biased information. It expresses its own point of view. \
                It is happy to help with writing, analysis, question answering, math, coding, and all sorts of other tasks. \
                Give the answers only in Russian Language.",
                self.strings["help_prompt"],
                validator=loader.validators.String(max_len=1024),
            ),
            loader.ConfigValue(
                "api_key",
                "",
                self.strings["api_key"],
                validator=loader.validators.Hidden(),
            ),
            loader.ConfigValue(
                "model",
                "llama3-8b-8192",
                self.strings["models"],
                validator=loader.validators.Choice(
                    [
                        "mixtral-8x7b-32768",
                        "gemma2-9b-it",
                        "gemma-7b-it",
                        "llama-3.1-70b-versatile",
                        "llama-3.1-8b-instant",
                        "llama3-70b-8192",
                        "llama3-8b-8192",
                        "llama-guard-3-8b",
                        "llama3-groq-70b-8192-tool-use-preview",
                        "llama3-groq-8b-8192-tool-use-preview",
                    ]
                ),
            ),
            loader.ConfigValue(
                "auto_tr",
                "False",
                "Ð­Ñ‚Ð¾ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐºÐ¸Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð½Ð° ÑÐ·Ñ‹Ðº Ð²Ð°ÑˆÐµÐ¹ Ñ…Ð¸ÐºÐºÐ¸.\nÐ’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚Ðµ Ð¿Ð¾ Ð½Ð°Ð´Ð¾Ð±Ð½Ð¾ÑÑ‚Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð°",
                validator=loader.validators.Boolean(),
            ),
        )

    @loader.command()
    async def ask(self, message: Message):
        """Ð—Ð°Ð´Ð°Ñ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð˜Ð˜"""
        question = utils.get_args_raw(message)

        if not self.config["api_key"]:
            await utils.answer(message, self.strings["api_key"])
            return
        if not question:
            await utils.answer(message, self.strings["q_err"])
            return

        await utils.answer(message, self.strings["loading"])

        response = self.ask_question(question)

        if self.config["auto_tr"]:
            await self.invoke("tr", response, edit=True, peer=message.peer_id)
        else:
            await utils.answer(message, response, parse_mode="md")

    def ask_question(self, question: str) -> str:
        try:
            response = post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.config['api_key']}",
                },
                json={
                    "messages": [
                        {"role": "system", "content": self.config["prompt"]},
                        {"role": "user", "content": question},
                    ],
                    "model": self.config["model"],
                    "temperature": 0.7,
                    "max_tokens": 1024,
                    "top_p": 0.9,
                    "stream": False,
                },
            )
            response.raise_for_status()
            data = response.json()
            return f"{self.strings['question']}`{question}`\n\nðŸ¤–: {data['choices'][0]['message']['content']}"
        except RequestException as e:
            return f"Error: {e}\n\n{self.strings['termux']}"